{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f4265f",
   "metadata": {},
   "source": [
    "#Diabetes Data Analysis \n",
    "*Author: Kirti Agarwal | Date: Aug 24, 2025*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a578cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset from kaggle\n",
    "import kagglehub\n",
    "\n",
    "#Downloading latest version\n",
    "path = kagglehub.dataset_download(\"mathchi/diabetes-data-set\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the diabetes.csv file \n",
    "df=pd.read_csv(\"dataset/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information of each column \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608339c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing easch coloumn data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a163e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the null no. of null values in each column \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01037437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the no. of 0 values in each column \n",
    "(df==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db241ae",
   "metadata": {},
   "source": [
    "### Handling Zero values in Critical Features \n",
    "\n",
    "**Glucose → Can’t be zero because the body always has some sugar in the blood.\n",
    "\n",
    "**BloodPressure → Can’t be zero because a living person must have circulating blood pressure.\n",
    "\n",
    "**SkinThickness → Can’t be zero because every person has some skin/fat thickness.\n",
    "\n",
    "**Insulin → Can’t be zero because the body naturally produces at least a small amount.\n",
    "\n",
    "**BMI → Can’t be zero because no person has zero weight or height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting four columns\n",
    "cols_with_zeros=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all the zeroes will null\n",
    "df[cols_with_zeros]=df[cols_with_zeros].replace(0,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab267f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist Count plot of each column \n",
    "df[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]].hist(figsize=(10,8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160dfdc",
   "metadata": {},
   "source": [
    "###Examining distribution to handle null values \n",
    "\n",
    "**Glucose → looks fairly symmetric (bell-like) → using mean to handle missing values.\n",
    "\n",
    "**BloodPressure → also looks roughly symmetric → using mean to handle missing values.\n",
    "\n",
    "**BMI → fairly symmetric → using mean to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with mean\n",
    "df['Glucose'] = df['Glucose'].fillna(df['Glucose'].mean())\n",
    "df['BloodPressure'] = df['BloodPressure'].fillna(df['BloodPressure'].mean())\n",
    "df['BMI'] = df['BMI'].fillna(df['BMI'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the no. of 0 values in each column \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52406e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing heatmap descibing correlation among each column \n",
    "\n",
    "# Select relevant features\n",
    "features_for_knn = ['Pregnancies', 'Glucose', 'BMI', 'Age', 'BloodPressure', 'SkinThickness', 'Insulin','DiabetesPedigreeFunction']\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df[features_for_knn].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6271b32",
   "metadata": {},
   "source": [
    "###Handling missing data for SkinThickeness and Insulin\n",
    "**For SkinThickness:**\n",
    "Strongest correlation is with BMI (0.65)\n",
    "**For Insulin:**\n",
    "Strongest correlation is with Glucose (0.58)\n",
    "This suggests that BMI and Glucose can be used as a reliable predictor when imputing missing values for these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KNN Imputer for handling missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Imputation for SkinThickness\n",
    "\n",
    "# Select features that are correlated with SkinThickness\n",
    "features_skin = ['BMI', 'Glucose', 'BloodPressure', 'SkinThickness']\n",
    "\n",
    "# Initialize the KNN Imputer with k=5 neighbors\n",
    "imputer_skin = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN imputation on the selected features\n",
    "imputed_skin = imputer_skin.fit_transform(df[features_skin])\n",
    "\n",
    "# Replace missing SkinThickness values with imputed results\n",
    "df['SkinThickness'] = imputed_skin[:, features_skin.index('SkinThickness')]\n",
    "\n",
    "# Imputation for Insulin\n",
    "\n",
    "# Select features that are correlated with Insulin\n",
    "features_insulin = ['Glucose', 'BMI', 'Age', 'Insulin']\n",
    "\n",
    "# Initialize another KNN Imputer with k=5 neighbors\n",
    "imputer_insulin = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN imputation on the selected features\n",
    "imputed_insulin = imputer_insulin.fit_transform(df[features_insulin])\n",
    "\n",
    "# Replace missing Insulin values with imputed results\n",
    "df['Insulin'] = imputed_insulin[:, features_insulin.index('Insulin')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70260220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the missing values have been replaced with knn inputation values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbaec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution after null values replacement \n",
    "import matplotlib.pyplot as plt\n",
    "df[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]].hist(figsize=(10,8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b87f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "# List of numeric columns\n",
    "cols = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]\n",
    "\n",
    "# Boxplots before handling outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "df[cols].boxplot()\n",
    "plt.title(\"Boxplot of Features (Before Handling Outliers)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation for insulin as it is highly skewed \n",
    "df[\"Insulin_log\"] = np.log1p(df[\"Insulin\"])  \n",
    "df.drop('Insulin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot before vs after log transformation\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df[[\"Insulin\"]].boxplot()\n",
    "plt.title(\"Insulin (Original)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df[[\"Insulin_log\"]].boxplot()\n",
    "plt.title(\"Insulin (Log Transformed)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric columns\n",
    "cols = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "\n",
    "# Boxplots of Skin thickness before handling outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "df[[\"SkinThickness\"]].boxplot()\n",
    "plt.title(\"Boxplot of Features (Before Handling Outliers)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ba55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Outlier Detection for SkinThickness\n",
    "# -----------------------------------------------\n",
    "# Note: Anything above ~50–60 mm for SkinThickness is considered very unlikely\n",
    "# based on medical/anthropometric studies.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to detect outliers using the IQR (Interquartile Range) method\n",
    "def detect(data, ST):\n",
    "    # Calculate the first quartile (Q1, 25th percentile)\n",
    "    q1 = data[ST].quantile(0.25)\n",
    "    \n",
    "    # Calculate the third quartile (Q3, 75th percentile)\n",
    "    q3 = data[ST].quantile(0.75)\n",
    "    \n",
    "    # Compute the Interquartile Range (IQR = Q3 - Q1)\n",
    "    IQR = q3 - q1\n",
    "    \n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = q1 - 1.5 * IQR\n",
    "    upper_bound = q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify rows where SkinThickness is outside the valid range\n",
    "    outliers = data[(data[ST] < lower_bound) | (data[ST] > upper_bound)]\n",
    "    \n",
    "    # Return detected outliers and boundary values\n",
    "    return outliers, q1, q3, lower_bound, upper_bound\n",
    "\n",
    "# Apply the outlier detection function on the 'SkinThickness' column\n",
    "outliers, q1, q3, lower_bound, upper_bound = detect(df, 'SkinThickness')\n",
    "\n",
    "# Print the total number of outliers detected\n",
    "print(\"The number of outliers present are:\", len(outliers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Removing Outliers in SkinThickness\n",
    "\n",
    "# Filter the dataset to keep only rows where SkinThickness <= 65\n",
    "# (Values above 65 mm are considered unrealistic based on medical context)\n",
    "df = df[df[\"SkinThickness\"] <= 65].copy()\n",
    "\n",
    "# Print the updated shape of the dataset after removing outliers\n",
    "print(\"Updated shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03260621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot after handling outliers in Skin Thickness\n",
    "plt.figure(figsize=(12, 6))\n",
    "df[[\"SkinThickness\"]].boxplot()\n",
    "plt.title(\"Boxplot of Features (Before Handling Outliers)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db426786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of DiabetesPedigreeFunction\n",
    "plt.figure(figsize=(12, 6))\n",
    "df[[\"DiabetesPedigreeFunction\"]].boxplot()\n",
    "plt.title(\"Boxplot of PDF\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6f788",
   "metadata": {},
   "source": [
    "### PDF values between 0.1 and 2.5 are perfectly reasonable.\n",
    "\n",
    "In our dataset, PDF values that fall between **0.1 and 2.5** are considered \n",
    "to be within a realistic and acceptable range.  \n",
    "Values inside this range are perfectly reasonable and do not indicate anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136f8e5",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Below we present visualizations to better understand the data distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b00ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# Plot distribution of all features except 'Outcome'\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Loop through all columns \n",
    "for col in df.columns[:-1]:\n",
    "    plt.figure(figsize=(6,4))  # set figure size for readability\n",
    "    \n",
    "    # Plot histogram with KDE (Kernel Density Estimate)\n",
    "    sns.histplot(df[col], kde=True, bins=30)\n",
    "    \n",
    "    # Add a clear title\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=14)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0008df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Outcome', data=df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Outcome', y='Glucose', data=df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4025304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a heatmap to visualize correlations between all numerical features\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1022e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot colored by Outcome\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x='Glucose', y='Insulin', hue='Outcome', data=df, palette={0:'blue', 1:'red'})\n",
    "plt.title('Glucose vs Insulin (Blue: Non-diabetic, Red: Diabetic)')\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('Insulin')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a41136",
   "metadata": {},
   "source": [
    "### Interpretation of Glucose vs. Insulin Plot\n",
    "\n",
    "- **Diabetics (red):**  \n",
    "  Tend to show higher glucose levels, along with either:  \n",
    "  - **Lower insulin levels** (indicative of Type 1 diabetes), or  \n",
    "  - **Higher insulin levels** (indicative of Type 2 diabetes).  \n",
    "\n",
    "- **Non-diabetics (blue):**  \n",
    "  Mostly cluster within a \"healthy range\" for both glucose and insulin values, \n",
    "  reflecting normal metabolic function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202fc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships between all features in the dataset\n",
    "# Color points by 'Outcome' to distinguish diabetics vs non-diabetics\n",
    "sns.pairplot(df, hue='Outcome')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33153938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BMI\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot BMI distribution for non-diabetic patients\n",
    "sns.kdeplot(data=df[df['Outcome']==0]['BMI'], label='Non-diabetic', fill=True, color='blue')\n",
    "\n",
    "# Plot BMI distribution for diabetic patients\n",
    "sns.kdeplot(data=df[df['Outcome']==1]['BMI'], label='Diabetic', fill=True, color='red')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title('BMI Distribution: Diabetic vs Non-diabetic')\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cd41d",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "**BMI Distribution Shift:**  \n",
    "The diabetic group (red) likely has a higher average BMI than the non-diabetic group (blue), as its density curve seems shifted to the right (toward higher BMI values).  \n",
    "This aligns with known medical trends: obesity (BMI ≥30) is a major risk factor for Type 2 diabetes.\n",
    "\n",
    "**Spread/Variability:**  \n",
    "The diabetic group may show wider variability in BMI (broader curve), suggesting greater diversity in body weight among diabetics.  \n",
    "The non-diabetic group’s curve is narrower, indicating most cluster around a lower BMI range.\n",
    "\n",
    "**Overlap:**  \n",
    "There’s significant overlap between groups, meaning BMI alone isn’t a perfect predictor of diabetes status (some non-diabetics have high BMIs, and vice versa).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Insulin\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot Insulin distribution for non-diabetic patients\n",
    "sns.kdeplot(data=df[df['Outcome']==0]['Insulin'], label='Non-diabetic', fill=True, color='blue')\n",
    "\n",
    "# Plot Insulin distribution for diabetic patients\n",
    "sns.kdeplot(data=df[df['Outcome']==1]['Insulin'], label='Diabetic', fill=True, color='red')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title('Insulin Distribution: Diabetic vs Non-diabetic')\n",
    "plt.xlabel('Insulin')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624ec89",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "**Diabetics Have Bimodal Insulin Distribution:**  \n",
    "The red curve (diabetic) has two peaks:  \n",
    "- One at lower insulin levels (~0–200 μU/mL), likely representing Type 1 diabetics (insulin deficiency).  \n",
    "- Another at higher insulin levels (~400–800 μU/mL), likely representing Type 2 diabetics (insulin resistance).\n",
    "\n",
    "**Non-Diabetics Have a Unimodal, Tighter Distribution:**  \n",
    "The blue curve (non-diabetic) peaks at moderate insulin levels (~50–150 μU/mL), reflecting normal metabolic function.  \n",
    "Their insulin range is narrower, with very few outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Glucose\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot Glucose distribution for non-diabetic patients\n",
    "sns.kdeplot(data=df[df['Outcome']==0]['Glucose'], label='Non-diabetic', fill=True, color='blue')\n",
    "\n",
    "# Plot Glucose distribution for diabetic patients\n",
    "sns.kdeplot(data=df[df['Outcome']==1]['Glucose'], label='Diabetic', fill=True, color='red')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title('Glucose Distribution: Diabetic vs Non-diabetic')\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728112bf",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "**Clear Separation Between Groups:**  \n",
    "The diabetic group (red) shows higher glucose levels overall, with a peak likely around 125–200 mg/dL (prediabetic/diabetic range).  \n",
    "The non-diabetic group (blue) peaks at a lower glucose range (~70–100 mg/dL), which is the normal fasting glucose range.\n",
    "\n",
    "**Minimal Overlap:**  \n",
    "Unlike BMI or insulin, glucose shows better separability between groups, making it a stronger diagnostic marker.  \n",
    "Overlap occurs around 100–125 mg/dL (prediabetic range), where some misclassification could happen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For DiabetesPedigreeFunction\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot DiabetesPedigreeFunction (DPF) distribution for non-diabetic patients\n",
    "sns.kdeplot(data=df[df['Outcome']==0]['DiabetesPedigreeFunction'], label='Non-diabetic', fill=True, color='blue')\n",
    "\n",
    "# Plot DiabetesPedigreeFunction (DPF) distribution for diabetic patients\n",
    "sns.kdeplot(data=df[df['Outcome']==1]['DiabetesPedigreeFunction'], label='Diabetic', fill=True, color='red')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title('DiabetesPedigreeFunction Distribution: Diabetic vs Non-diabetic')\n",
    "plt.xlabel('DiabetesPedigreeFunction')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c102c75",
   "metadata": {},
   "source": [
    "## Key Observation\n",
    "\n",
    "Even people with high genetic risk (DiabetesPedigreeFunction, DPF) might not develop diabetes due to lifestyle factors (e.g., exercise, diet).  \n",
    "The plot above shows that both diabetics (red) and non-diabetics (blue) have overlapping DPF values, indicating that genetics alone does not fully determine diabetes risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a117fa",
   "metadata": {},
   "source": [
    "## Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"Outcome\", axis=1)  # X = all features\n",
    "y = df[\"Outcome\"]               # y = target column\n",
    "\n",
    "# Build the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "# Sort and plot importance of each feature\n",
    "importances.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1802b2f1",
   "metadata": {},
   "source": [
    "#### Based on feature importance, Glucose, BMI, Insulin, Age, etc. are the most predictive features.  \n",
    "#### Since all features contribute some information, we will use all features in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7c37e",
   "metadata": {},
   "source": [
    "**Data Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Split into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be7ef5",
   "metadata": {},
   "source": [
    "**Data Training using Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build the model\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "# Train it\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f266895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction \n",
    "y_pred = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242500bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7daeb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build model with class balancing\n",
    "rf_model_balanced = RandomForestClassifier(\n",
    "    random_state=42, \n",
    "    n_estimators=100, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf_model_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_balanced = rf_model_balanced.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_balanced))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_balanced))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_balanced))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_balanced))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_balanced))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_balanced)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2dd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a hyperparameter grid for Random Forest (number of trees, depth, min samples, class weights).\n",
    "# Use GridSearchCV with 5-fold cross-validation to find the best Random Forest model based on F1 score.\n",
    "# Fit the best model on the resampled training data.\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Train-test split (stratify to maintain class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Random Forest with hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4856b0",
   "metadata": {},
   "source": [
    "### Model Development Strategy\n",
    "\n",
    "After experimenting with **Random Forest hyperparameter tuning** using `GridSearchCV`, we observed **no significant improvement** in model performance.  \n",
    "\n",
    "Therefore, instead of further tuning a single model, we will proceed to **train multiple machine learning models** (e.g., Logistic Regression, Decision Tree, Random Forest, XGBoost) and **compare their accuracy and other evaluation metrics**. The goal is to identify the model that performs best on our diabetes prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ---------------------------\n",
    "# Features and target\n",
    "# ---------------------------\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# ---------------------------\n",
    "# Train-test split\n",
    "# ---------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Scale numeric features\n",
    "# ---------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "# Apply SMOTE\n",
    "# ---------------------------\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.7)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# ---------------------------\n",
    "# Define models to compare\n",
    "# ---------------------------\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Train, predict, and evaluate\n",
    "# ---------------------------\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1\n",
    "    })\n",
    "\n",
    "# ---------------------------\n",
    "# Convert results to DataFrame\n",
    "# ---------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"F1-score\", ascending=False).reset_index(drop=True)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f17a0",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "- Among all models, **AdaBoost achieved the highest Accuracy (72.7%) and F1-score (0.604)**, indicating better overall performance in correctly predicting diabetic and non-diabetic cases.  \n",
    "- Therefore, **AdaBoost is selected as the best-performing model** for this diabetes prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fde5ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Insulin'], inplace=True)\n",
    "# Rename the column\n",
    "df.rename(columns={'Insulin_log': 'Insulin'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# ---------------------------\n",
    "# Features and target\n",
    "# ---------------------------\n",
    "\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# ---------------------------\n",
    "# Train-test split\n",
    "# ---------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Scale numeric features\n",
    "# ---------------------------\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "# Apply SMOTE\n",
    "# ---------------------------\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.7)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# ---------------------------\n",
    "# Cross-validation (initial AdaBoost)\n",
    "# ---------------------------\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)  # default weak learner\n",
    "ada = AdaBoostClassifier(estimator=base_estimator, random_state=42)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(ada, X_train_res, y_train_res, cv=cv, scoring='f1')\n",
    "print(\"5-Fold CV F1-scores:\", cv_scores)\n",
    "print(\"Mean CV F1-score:\", cv_scores.mean())\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameter tuning (Grid Search)\n",
    "# ---------------------------\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'estimator__max_depth': [1, 2, 3]  # tune the weak tree depth\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ada,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',  # focus on class 1 performance\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Best AdaBoost model\n",
    "best_ada = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# ---------------------------\n",
    "# Final evaluation on test set\n",
    "# ---------------------------\n",
    "y_pred = best_ada.predict(X_test_scaled)\n",
    "y_prob = best_ada.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93bf71",
   "metadata": {},
   "source": [
    "### AdaBoost Model Development and Hyperparameter Tuning\n",
    "\n",
    "1. **Data Preparation and Scaling**  \n",
    "   - Features (`X`) and target (`Outcome`) were separated from the dataset.  \n",
    "   - Data was split into training (80%) and testing (20%) sets using stratification to preserve class distribution.  \n",
    "   - Numeric features were standardized using `StandardScaler` to improve model performance.  \n",
    "   - SMOTE was applied on the training data to address class imbalance, generating synthetic samples for the minority class.\n",
    "\n",
    "2. **Initial Model Evaluation (Cross-Validation)**  \n",
    "   - An initial **AdaBoost** model was trained using a **DecisionTreeClassifier with max_depth=1** as the weak learner.  \n",
    "   - **5-fold Stratified Cross-Validation** was performed on the resampled training data to evaluate baseline F1-score.  \n",
    "   - This step ensured that the model performance was stable across different splits.\n",
    "\n",
    "3. **Hyperparameter Tuning (Grid Search)**  \n",
    "   - Grid search was performed to tune the following parameters:\n",
    "     - `n_estimators`: number of boosting rounds  \n",
    "     - `learning_rate`: step size for updating weights  \n",
    "     - `estimator__max_depth`: depth of each weak decision tree  \n",
    "   - `GridSearchCV` with 5-fold stratified CV was used, optimizing the **F1-score** for the minority class.  \n",
    "   - The best hyperparameters were selected to improve model performance.\n",
    "\n",
    "4. **Final Model Evaluation**  \n",
    "   - The best AdaBoost model was evaluated on the **test set**.  \n",
    "   - Metrics reported include **Accuracy, ROC-AUC, and detailed Classification Report** (Precision, Recall, F1-score).  \n",
    "   - This ensures that the model generalizes well to unseen data and balances class-specific performance.\n",
    "\n",
    "**Summary:**  \n",
    "This workflow combines **data balancing (SMOTE)**, **feature scaling**, **cross-validation**, and **hyperparameter tuning** to train a robust AdaBoost model for diabetes prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83248934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving AdaBoost model using pickle\n",
    "with open(\"ada_classification_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_ada, f)\n",
    "\n",
    "# Saving StandardScaler using pickle \n",
    "with open(\"ada_scaler_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8b376",
   "metadata": {},
   "source": [
    "### Saving the Model and Scaler\n",
    "\n",
    "I saved the trained **AdaBoost model** and the **StandardScaler** using `pickle` so they can be reused later without retraining.  \n",
    "\n",
    "- `ada_classification_model.pkl` → stores the trained AdaBoost model  \n",
    "- `ada_scaler_model.pkl` → stores the fitted scaler for consistent feature scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d23aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
